{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bc4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f158f1a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01macquired\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mexplore\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mprepare\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmodel\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m token, username\n\u001b[1;32m     17\u001b[0m languages \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/codeup-data-science/nlp-project/Py Files/acquired.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m token, username\n\u001b[1;32m     12\u001b[0m repos \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Global variable for repository names\u001b[39;00m\n\u001b[1;32m     13\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'env'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import acquired, explore, prepare, model\n",
    "from env import token, username\n",
    "\n",
    "languages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571443f1",
   "metadata": {},
   "source": [
    "> Create a data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.custom_visual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c803d",
   "metadata": {},
   "source": [
    "> Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquired.get_github_data(token, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a004b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data_first_look.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2344d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame to a CSV file\n",
    "df = pd.read_csv('readme_data_first_look.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab888db5",
   "metadata": {},
   "source": [
    "> Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.clean_readme_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2234e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.process_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.preprocess_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ffaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26923b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and pass your DataFrame as the argument\n",
    "prepare.apply_stopword_removal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d709774",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.extract_common_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ac437",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.apply_pos_tagging_and_lemmatization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12543e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d00b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of words in the 'Readme' column and select the top 10 most frequent words\n",
    "freq = pd.Series(' '.join(df['Readme']).split()).value_counts()[:10]\n",
    "\n",
    "# Convert the frequency series to a list of words\n",
    "freq = list(freq.index)\n",
    "\n",
    "# Remove the top 10 most frequent words from the 'Readme' column\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14448b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of each word in the 'Readme' column\n",
    "freq = pd.Series(' '.join(df['Readme']).split()).value_counts()\n",
    "\n",
    "# Select the 10 least frequent words\n",
    "freq = freq[-10:]\n",
    "\n",
    "# Convert the index (words) into a list\n",
    "freq = list(freq.index)\n",
    "\n",
    "# Remove the least frequent words from the 'Readme' column\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame to a CSV file\n",
    "df = pd.read_csv('readme_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28befa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a62a45",
   "metadata": {},
   "source": [
    "> Retrieved 100 of the most starred repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.analyze_readme_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5552c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = explore.merge_analyze_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b573a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235796e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ccef1",
   "metadata": {},
   "source": [
    "> Scraped the README files from each repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5399f",
   "metadata": {},
   "source": [
    "Performs web scraping to gather README files from a list of GitHub repositories. Let's go through the code and its output step by step:\n",
    "\n",
    "The code imports the required libraries and modules: datetime, requests, pandas, and the token and username variables from the env.py file.\n",
    "The repos list contains the names of the repositories from which we want to scrape README files.\n",
    "The readme_data dictionary is initialized to store the repository names and their corresponding README contents.\n",
    "8-19. The code iterates over each repository in the repos list. It constructs the URL for the repository's README file using the GitHub API. It sends a GET request to the URL with the necessary authentication headers. If the response is successful (status code 200), it extracts the content of the README file.\n",
    "\n",
    "12-15. The base64-encoded content is decoded and converted to UTF-8 format to obtain the plain text of the README file.\n",
    "\n",
    "18-20. The repository name and the corresponding README content are appended to the readme_data dictionary.\n",
    "\n",
    "23-26. Once all the repositories have been processed, the readme_data dictionary is converted into a DataFrame using the pd.DataFrame() function.\n",
    "\n",
    "The DataFrame is saved to a CSV file named 'readme_data.csv' using the to_csv() method.\n",
    "32-34. The code prints a notification message indicating the date when the data was collected. The datetime.now() function retrieves the current date, which is then formatted as \"%Y-%m-%d\" (e.g., \"2023-05-14\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24537569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86094b49",
   "metadata": {},
   "source": [
    "performs various operations related to scraping and analyzing README files from GitHub repositories. Let's go through the code and its output step by step:\n",
    "\n",
    "1-4: The code imports the required libraries and modules: requests, pandas, re, Counter, and the token and username variables from the env.py file.\n",
    "\n",
    "7-16: The code defines the GitHub API endpoint and sets the parameters for the API request. These parameters specify the search query, sorting criteria, order, and number of repositories to retrieve.\n",
    "\n",
    "18-21: Authentication headers are set using the token and username variables to authorize the request.\n",
    "\n",
    "24-29: The code sends a GET request to the GitHub API with the specified URL, parameters, and authentication headers.\n",
    "\n",
    "32-40: The response status code is checked to ensure that the request was successful (status code 200). If successful, the repository names are extracted from the response data.\n",
    "\n",
    "43-54: A list of programming-related words is defined. These words represent common programming languages.\n",
    "\n",
    "57-59: A dictionary called word_categories is created to categorize words by programming language. Each word is initially associated with an empty list.\n",
    "\n",
    "62-80: The code iterates over each repository in the retrieved list of repositories. For each repository, it constructs the URL for the README file using the GitHub API. It sends a GET request to the URL with the necessary authentication headers. If the response is successful, it extracts the content of the README file.\n",
    "\n",
    "83-89: The base64-encoded content is decoded and converted to UTF-8 format to obtain the plain text of the README file.\n",
    "\n",
    "92-99: The code uses regular expressions (re.findall()) to extract individual words from the README text. These words are converted to lowercase.\n",
    "\n",
    "102-106: The words are categorized by programming language. If a word exists in the word_categories dictionary, the repository is added to the corresponding list.\n",
    "\n",
    "109-112: The code finds the most common words in the READMEs using the Counter class from the collections module. The most_common() method is used to retrieve the most common words and their counts.\n",
    "\n",
    "115-121: The most common words are displayed, along with their counts.\n",
    "\n",
    "The output of this code will be the most common words found in the READMEs, along with their respective frequencies. For example:\n",
    "\n",
    "Most common words in READMEs:\n",
    "the: 166\n",
    "cheat: 149\n",
    "sh: 146\n",
    "you: 115\n",
    "a: 101\n",
    "to: 93\n",
    "cht: 81\n",
    "and: 62\n",
    "in: 62\n",
    "https: 60\n",
    "\n",
    "These words indicate the frequency at which they appear in the README files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084d2be",
   "metadata": {},
   "source": [
    "> Need to create a fucntion for the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6734807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "explore.visualize_top_word(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f83743",
   "metadata": {},
   "source": [
    "> Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc2c29",
   "metadata": {},
   "source": [
    "> Clean Me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9221d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, common_words, average_length_by_language = explore.analyze_readme_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for analysis\n",
    "length_by_language = {}\n",
    "unique_words_by_language = {}\n",
    "unique_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'df' with the 'Average README Length' column\n",
    "average_length_by_language = df['Average README Length'].groupby(df['Language']).mean().to_dict()\n",
    "\n",
    "# Create the 'languages' and 'lengths' variables\n",
    "languages = list(average_length_by_language.keys())\n",
    "lengths = list(average_length_by_language.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.plot_average_length_by_language(languages, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f558bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.plot_average_length_by_language_barh(languages, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame to a CSV file\n",
    "df = pd.read_csv('readme_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cleaned dataset\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccac88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_github_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dad9eb",
   "metadata": {},
   "source": [
    "> Ready for Explore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and pass your DataFrame as the argument\n",
    "model.train_and_evaluate_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e52a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68556e0f",
   "metadata": {},
   "source": [
    "Performs a text classification task using machine learning algorithms. The goal is to predict the programming language of a repository based on the contents of its README file.\n",
    "\n",
    "Here's an explanation of the code and the resulting classification report:\n",
    "\n",
    "The code starts by splitting the data into training and testing sets. The README content (df['Readme']) is used as the input features, and the corresponding programming language (df['Language']) is the target variable. The data is divided into 80% training set and 20% testing set.\n",
    "4-6. The TF-IDF vectorizer (TfidfVectorizer()) is created. This is a technique that converts text documents into numerical vectors, representing the importance of words in the documents.\n",
    "\n",
    "8-9. The vectorizer is fitted on the training data (X_train) to learn the vocabulary and calculate the TF-IDF weights for the words.\n",
    "\n",
    "11-12. The testing data (X_test) is transformed using the fitted vectorizer, resulting in numerical vectors that represent the README contents.\n",
    "\n",
    "14-15. A Support Vector Machine (SVM) classifier (SVC()) is created. SVM is a machine learning algorithm used for classification tasks.\n",
    "\n",
    "The SVM classifier is trained on the training data (X_train_vectors, y_train), where it learns patterns and relationships between the README contents and their corresponding programming languages.\n",
    "The trained SVM classifier is used to make predictions on the testing data (X_test_vectors).\n",
    "The classification_report function is called to generate a report that evaluates the performance of the classification. It calculates various metrics such as precision, recall, F1-score, and support for each programming language. The zero_division=1 parameter is set to handle the case where a programming language has no predicted samples.\n",
    "The resulting classification report shows the performance metrics for each programming language. Precision represents the accuracy of the predictions for each language, recall measures the coverage of the predictions, and F1-score is a combination of precision and recall. The support column indicates the number of samples in the testing set for each language.\n",
    "\n",
    "Overall, the classification report shows the performance of the model in predicting the programming languages based on the README contents. The accuracy is given as 0.79, which means that the model predicted the correct programming language for 79% of the repositories in the testing set. The precision, recall, and F1-score values vary for different programming languages, indicating how well the model performs for each specific language. The macro avg and weighted avg values provide an overall evaluation of the model's performance across all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your DataFrame 'df'\n",
    "explore.add_word_count_and_average_length(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453309f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your DataFrame 'df'\n",
    "prepare.extracted_ngrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your DataFrame 'df'\n",
    "explore.visualize_top_bigrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e924e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_and_evaluate_models(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your DataFrame 'df' and the column name 'Language'\n",
    "explore.plot_language_distribution(df, 'Language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170e10a",
   "metadata": {},
   "source": [
    "> Final Test...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_models(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1a447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
