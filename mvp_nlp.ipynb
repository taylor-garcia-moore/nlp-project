{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a040d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import prepare\n",
    "from env import token, username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.custom_visual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c803d",
   "metadata": {},
   "source": [
    "> Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from env import token, username\n",
    "\n",
    "# GitHub API endpoint\n",
    "url = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    \"q\": \"stars:>0\",\n",
    "    \"sort\": \"stars\",\n",
    "    \"order\": \"desc\",\n",
    "    \"per_page\": 100,\n",
    "}\n",
    "\n",
    "# Authentication headers\n",
    "headers = {\"Authorization\": f\"token {token}\", \"User-Agent\": username}\n",
    "\n",
    "# Send a GET request to the GitHub API\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # Get the response data\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Extract the repository names\n",
    "    repos = [repo[\"full_name\"] for repo in response_data[\"items\"]]\n",
    "\n",
    "    # Display the repository names\n",
    "    print(repos)\n",
    "else:\n",
    "    print(\"Failed to retrieve repository names from the GitHub API\")\n",
    "    \n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert repos list to DataFrame\n",
    "df_repos = pd.DataFrame({'Repository': repos})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703c8ea",
   "metadata": {},
   "source": [
    "> Retrieved 100 of the most starred repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from env import token, username\n",
    "\n",
    "# List of repository names\n",
    "#df_repos = repos\n",
    "#repos = ['trekhleb/javascript-algorithms', 'ohmyzsh/ohmyzsh', 'TheAlgorithms/Python', 'torvalds/linux', 'Snailclimb/JavaGuide', 'huggingface/transformers', 'trimstray/the-book-of-secret-knowledge', 'microsoft/PowerToys', 'rust-lang/rust', 'f/awesome-chatgpt-prompts', 'ripienaar/free-for-dev', 'bitcoin/bitcoin', 'spring-projects/spring-boot', 'netdata/netdata', 'coder/code-server', 'reduxjs/redux', 'atom/atom', 'protocolbuffers/protobuf', 'angular/angular.js', 'keras-team/keras', 'h5bp/Front-end-Developer-Interview-Questions', 'nestjs/nest', 'lodash/lodash', 'leonardomso/33-js-concepts', 'apache/echarts', 'rails/rails', 'MunGell/awesome-for-beginners', 'scutan90/DeepLearning-500-questions', 'supabase/supabase', 'google/material-design-icons', 'nuxt/nuxt', 'serverless/serverless', 'ryanoasis/nerd-fonts', 'meteor/meteor', 'cypress-io/cypress', 'Textualize/rich', 'iamkun/dayjs', 'ColorlibHQ/AdminLTE', 'parcel-bundler/parcel', 'ethereum/go-ethereum', 'styled-components/styled-components', 'youngyangyang04/leetcode-master', 'discourse/discourse', 'agalwood/Motrix', 'hashicorp/terraform', 'vuetifyjs/vuetify', 'FFmpeg/FFmpeg', 'serhii-londar/open-source-mac-os-apps', 'jesseduffield/lazygit', 'wg/wrk', 'ant-design/ant-design-pro', 'eugenp/tutorials', 'microsoft/monaco-editor', 'google/leveldb', 'LAION-AI/Open-Assistant', 'gto76/python-cheatsheet', 'alibaba/arthas', 'xitu/gold-miner', 'facebookresearch/segment-anything', 'psf/black', 'geekxh/hello-algorithm', 'pyenv/pyenv', 'withastro/astro', 'rapid7/metasploit-framework', 'mqyqingfeng/Blog', 'typicode/husky', 'lib-pku/libpku', 'bradtraversy/50projects50days', 'PKUanonym/REKCARC-TSC-UHT', 'ccxt/ccxt', 'jashkenas/backbone', 'solidjs/solid', 'postcss/postcss', 'cockroachdb/cockroach', 'fzaninotto/Faker', 'danielgindi/Charts', 'ibraheemdev/modern-unix', 'realpython/python-guide', 'nvie/gitflow', 'geekcompany/ResumeSample', 'explosion/spaCy', 'ehang-io/nps', 'ajaxorg/ace', 'encode/django-rest-framework', 'viraptor/reverse-interview', 'AllThingsSmitty/css-protips', 'yichengchen/clashX', 'ggreer/the_silver_searcher', 'Ebazhanov/linkedin-skill-assessments-quizzes', 'facebookresearch/fastText', 'akveo/ngx-admin', 'open-mmlab/mmdetection', 'pypa/pipenv', 'hammerjs/hammer.js', 'VundleVim/Vundle.vim', 'quasarframework/quasar', 'zhiwehu/Python-programming-exercises', 'qier222/YesPlayMusic', 'remix-run/remix', 'dnSpy/dnSpy']\n",
    "\n",
    "\n",
    "# Scrape README files and store their content in a dictionary\n",
    "readme_data = {'Repository': [], 'Readme': []}\n",
    "for repo in repos:\n",
    "    # Get the URL of the README file\n",
    "    url = f\"https://api.github.com/repos/{repo}/readme\"\n",
    "\n",
    "    # Send a GET request to the URL with authentication\n",
    "    headers = {\"Authorization\": f\"token {token}\", \"User-Agent\": username}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the content from the response\n",
    "        response_data = response.json()\n",
    "        readme_content = response_data['content']\n",
    "\n",
    "        # Decode the base64 encoded content\n",
    "        import base64\n",
    "        readme_text = base64.b64decode(readme_content).decode('utf-8')\n",
    "\n",
    "        # Store the repository name and the corresponding README content\n",
    "        readme_data['Repository'].append(repo)\n",
    "        readme_data['Readme'].append(readme_text)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve README for repository: {repo}\")\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(readme_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)\n",
    "\n",
    "# Print the notification message\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Our data comes from the top 100 trending repositories on GitHub as of {current_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5399f",
   "metadata": {},
   "source": [
    "Performs web scraping to gather README files from a list of GitHub repositories. Let's go through the code and its output step by step:\n",
    "\n",
    "The code imports the required libraries and modules: datetime, requests, pandas, and the token and username variables from the env.py file.\n",
    "The repos list contains the names of the repositories from which we want to scrape README files.\n",
    "The readme_data dictionary is initialized to store the repository names and their corresponding README contents.\n",
    "8-19. The code iterates over each repository in the repos list. It constructs the URL for the repository's README file using the GitHub API. It sends a GET request to the URL with the necessary authentication headers. If the response is successful (status code 200), it extracts the content of the README file.\n",
    "\n",
    "12-15. The base64-encoded content is decoded and converted to UTF-8 format to obtain the plain text of the README file.\n",
    "\n",
    "18-20. The repository name and the corresponding README content are appended to the readme_data dictionary.\n",
    "\n",
    "23-26. Once all the repositories have been processed, the readme_data dictionary is converted into a DataFrame using the pd.DataFrame() function.\n",
    "\n",
    "The DataFrame is saved to a CSV file named 'readme_data.csv' using the to_csv() method.\n",
    "32-34. The code prints a notification message indicating the date when the data was collected. The datetime.now() function retrieves the current date, which is then formatted as \"%Y-%m-%d\" (e.g., \"2023-05-14\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24537569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0259bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86094b49",
   "metadata": {},
   "source": [
    "performs various operations related to scraping and analyzing README files from GitHub repositories. Let's go through the code and its output step by step:\n",
    "\n",
    "1-4: The code imports the required libraries and modules: requests, pandas, re, Counter, and the token and username variables from the env.py file.\n",
    "\n",
    "7-16: The code defines the GitHub API endpoint and sets the parameters for the API request. These parameters specify the search query, sorting criteria, order, and number of repositories to retrieve.\n",
    "\n",
    "18-21: Authentication headers are set using the token and username variables to authorize the request.\n",
    "\n",
    "24-29: The code sends a GET request to the GitHub API with the specified URL, parameters, and authentication headers.\n",
    "\n",
    "32-40: The response status code is checked to ensure that the request was successful (status code 200). If successful, the repository names are extracted from the response data.\n",
    "\n",
    "43-54: A list of programming-related words is defined. These words represent common programming languages.\n",
    "\n",
    "57-59: A dictionary called word_categories is created to categorize words by programming language. Each word is initially associated with an empty list.\n",
    "\n",
    "62-80: The code iterates over each repository in the retrieved list of repositories. For each repository, it constructs the URL for the README file using the GitHub API. It sends a GET request to the URL with the necessary authentication headers. If the response is successful, it extracts the content of the README file.\n",
    "\n",
    "83-89: The base64-encoded content is decoded and converted to UTF-8 format to obtain the plain text of the README file.\n",
    "\n",
    "92-99: The code uses regular expressions (re.findall()) to extract individual words from the README text. These words are converted to lowercase.\n",
    "\n",
    "102-106: The words are categorized by programming language. If a word exists in the word_categories dictionary, the repository is added to the corresponding list.\n",
    "\n",
    "109-112: The code finds the most common words in the READMEs using the Counter class from the collections module. The most_common() method is used to retrieve the most common words and their counts.\n",
    "\n",
    "115-121: The most common words are displayed, along with their counts.\n",
    "\n",
    "The output of this code will be the most common words found in the READMEs, along with their respective frequencies. For example:\n",
    "\n",
    "Most common words in READMEs:\n",
    "the: 166\n",
    "cheat: 149\n",
    "sh: 146\n",
    "you: 115\n",
    "a: 101\n",
    "to: 93\n",
    "cht: 81\n",
    "and: 62\n",
    "in: 62\n",
    "https: 60\n",
    "\n",
    "These words indicate the frequency at which they appear in the README files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d95b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8333617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to categorize words by programming language\n",
    "word_categories = {word: [] for word in languages}\n",
    "\n",
    "# Variables for analysis\n",
    "length_by_language = {}\n",
    "unique_words_by_language = {}\n",
    "unique_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28816f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3db883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    headers = {\"Authorization\": f\"token {token}\", \"User-Agent\": username}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        repo_info = response.json()\n",
    "        if \"language\" in repo_info:\n",
    "            language = repo_info[\"language\"]\n",
    "            return language\n",
    "    return \"\"\n",
    "\n",
    "for repo in repos:\n",
    "    language = get_repo_language(repo)\n",
    "    if language:\n",
    "        print(f\"Repository: {repo}, Language: {language}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve language for repository: {repo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the languages\n",
    "languages = []\n",
    "\n",
    "# Loop through each repository in the DataFrame\n",
    "for repo in df['Repository']:\n",
    "    # Get the language for the repository\n",
    "    language = get_repo_language(repo)\n",
    "    # Append the language to the list\n",
    "    languages.append(language)\n",
    "\n",
    "# Create a new 'Language' column in the DataFrame\n",
    "df['Language'] = languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c51848",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary to categorize words by programming language\n",
    "word_categories = {word: [] for word in languages}\n",
    "\n",
    "# Create a dictionary to store the length of READMEs by language\n",
    "length_by_language = {}\n",
    "\n",
    "# Loop through each repository in the DataFrame\n",
    "for repo in df['Repository']:\n",
    "    # Get the URL of the README file\n",
    "    url = f\"https://api.github.com/repos/{repo}/readme\"\n",
    "\n",
    "    # Send a GET request to the URL with authentication\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the content from the response\n",
    "        response_data = response.json()\n",
    "        readme_content = response_data[\"content\"]\n",
    "\n",
    "        # Decode the base64 encoded content\n",
    "        import base64\n",
    "        readme_text = base64.b64decode(readme_content).decode(\"utf-8\")\n",
    "\n",
    "        # Extract words from the README text\n",
    "        words = re.findall(r\"\\b\\w+\\b\", readme_text.lower())\n",
    "\n",
    "        # Calculate the length of the README\n",
    "        length = len(readme_text)\n",
    "\n",
    "        # Update length by language dictionary\n",
    "        language = get_repo_language(repo)\n",
    "        if language in length_by_language:\n",
    "            length_by_language[language].append(length)\n",
    "        else:\n",
    "            length_by_language[language] = [length]\n",
    "\n",
    "        # Categorize words by programming language\n",
    "        for word in words:\n",
    "            if word in word_categories:\n",
    "                word_categories[word].append(repo)\n",
    "\n",
    "# Find the most common words in READMEs\n",
    "common_words = Counter(words).most_common(10)\n",
    "\n",
    "# Display the most common words\n",
    "print(\"Most common words in READMEs:\")\n",
    "for word, count in common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Calculate the average length of READMEs by programming language\n",
    "average_length_by_language = {\n",
    "    language: sum(lengths) / len(lengths)\n",
    "    for language, lengths in length_by_language.items()\n",
    "}\n",
    "\n",
    "# Display the average length of READMEs by programming language\n",
    "print(\"Average README Length by Language:\")\n",
    "for language, length in average_length_by_language.items():\n",
    "    print(f\"{language}: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8caf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [word for word, count in common_words]\n",
    "counts = [count for word, count in common_words]\n",
    "\n",
    "plt.barh(words, counts)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Most Common Words in READMEs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [word for word, count in common_words]\n",
    "counts = [count for word, count in common_words]\n",
    "\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most Common Words in READMEs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ef29f",
   "metadata": {},
   "source": [
    "> Clean Me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "readme_df = pd.DataFrame({\n",
    "    'Language': list(average_length_by_language.keys()),\n",
    "    'Average README Length': list(average_length_by_language.values())\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(readme_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b071246",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_length_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04085bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ef826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for analysis\n",
    "length_by_language = {}\n",
    "unique_words_by_language = {}\n",
    "unique_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faefca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "languages = list(average_length_by_language.keys())\n",
    "lengths = list(average_length_by_language.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032af250",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out None values from languages and lengths\n",
    "languages_filtered = [lang for lang in languages if lang is not None]\n",
    "lengths_filtered = [length for lang, length in zip(languages, lengths) if lang is not None]\n",
    "\n",
    "plt.plot(languages_filtered, lengths_filtered, marker='o')\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Average README Length')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8009fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out None values from languages and lengths\n",
    "languages_filtered = [lang for lang in languages if lang is not None]\n",
    "lengths_filtered = [length for lang, length in zip(languages, lengths) if lang is not None]\n",
    "\n",
    "plt.bar(languages_filtered, lengths_filtered)\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Average README Length')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(languages_filtered, lengths_filtered)\n",
    "plt.xlabel('Average README Length')\n",
    "plt.ylabel('Programming Language')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3beda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(languages_filtered, lengths_filtered)\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Average README Length')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4263dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de624644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c394f1b",
   "metadata": {},
   "source": [
    "> Data Acquired! Cleaning started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e95013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Readme'] = df['Readme'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Readme'] = df['Readme'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the default set of stopwords\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Add prepositions to the stopwords set\n",
    "prepositions = ['about', 'above', 'across', 'after', 'against', 'along', 'among', 'around',\n",
    "                'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between',\n",
    "                'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down',\n",
    "                'during', 'except', 'for', 'from', 'in', 'inside', 'into', 'like',\n",
    "                'near', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over', 'past',\n",
    "                'regarding', 'round', 'since', 'through', 'throughout', 'to', 'toward',\n",
    "                'under', 'underneath', 'until', 'unto', 'up', 'upon', 'with', 'within',\n",
    "                'without','htttp','https','com']\n",
    "\n",
    "stopwords_with_prepositions = default_stopwords.union(prepositions)\n",
    "\n",
    "# Print the stopwords with prepositions\n",
    "print(stopwords_with_prepositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75954c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the default set of stopwords\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Add prepositions to the stopwords set\n",
    "prepositions = ['about', 'above', 'across', 'after', 'against', 'along', 'among', 'around',\n",
    "                'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between',\n",
    "                'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down',\n",
    "                'during', 'except', 'for', 'from', 'in', 'inside', 'into', 'like',\n",
    "                'near', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over', 'past',\n",
    "                'regarding', 'round', 'since', 'through', 'throughout', 'to', 'toward',\n",
    "                'under', 'underneath', 'until', 'unto', 'up', 'upon', 'with', 'within',\n",
    "                'without','http','https','com']\n",
    "\n",
    "stopwords_with_prepositions = default_stopwords.union(prepositions)\n",
    "\n",
    "# Print the stopwords with prepositions\n",
    "print(stopwords_with_prepositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd187a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords removal to 'Readme' column\n",
    "stop = set(stopwords_with_prepositions)\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df['Readme']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df['Readme']).split()).value_counts()[-10:]\n",
    "freq = list(freq.index)\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0625cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d13fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out stop words from common words\n",
    "words = [word for word, count in common_words if word not in stopwords_with_prepositions]\n",
    "counts = [count for word, count in common_words if word not in stopwords_with_prepositions]\n",
    "\n",
    "plt.barh(words, counts)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Most Common Words in READMEs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c85269",
   "metadata": {},
   "source": [
    "> Ready for Explore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Drop rows with missing values in 'Language' column\n",
    "df = df.dropna(subset=['Language'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Remove rows with 'None' values from training data\n",
    "non_null_indices = y_train.notnull()\n",
    "X_train_vectors = X_train_vectors[non_null_indices]\n",
    "y_train = y_train[non_null_indices]\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report with zero_division=1\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e52a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68556e0f",
   "metadata": {},
   "source": [
    "Performs a text classification task using machine learning algorithms. The goal is to predict the programming language of a repository based on the contents of its README file.\n",
    "\n",
    "Here's an explanation of the code and the resulting classification report:\n",
    "\n",
    "The code starts by splitting the data into training and testing sets. The README content (df['Readme']) is used as the input features, and the corresponding programming language (df['Language']) is the target variable. The data is divided into 80% training set and 20% testing set.\n",
    "4-6. The TF-IDF vectorizer (TfidfVectorizer()) is created. This is a technique that converts text documents into numerical vectors, representing the importance of words in the documents.\n",
    "\n",
    "8-9. The vectorizer is fitted on the training data (X_train) to learn the vocabulary and calculate the TF-IDF weights for the words.\n",
    "\n",
    "11-12. The testing data (X_test) is transformed using the fitted vectorizer, resulting in numerical vectors that represent the README contents.\n",
    "\n",
    "14-15. A Support Vector Machine (SVM) classifier (SVC()) is created. SVM is a machine learning algorithm used for classification tasks.\n",
    "\n",
    "The SVM classifier is trained on the training data (X_train_vectors, y_train), where it learns patterns and relationships between the README contents and their corresponding programming languages.\n",
    "The trained SVM classifier is used to make predictions on the testing data (X_test_vectors).\n",
    "The classification_report function is called to generate a report that evaluates the performance of the classification. It calculates various metrics such as precision, recall, F1-score, and support for each programming language. The zero_division=1 parameter is set to handle the case where a programming language has no predicted samples.\n",
    "The resulting classification report shows the performance metrics for each programming language. Precision represents the accuracy of the predictions for each language, recall measures the coverage of the predictions, and F1-score is a combination of precision and recall. The support column indicates the number of samples in the testing set for each language.\n",
    "\n",
    "Overall, the classification report shows the performance of the model in predicting the programming languages based on the README contents. The accuracy is given as 0.79, which means that the model predicted the correct programming language for 79% of the repositories in the testing set. The precision, recall, and F1-score values vary for different programming languages, indicating how well the model performs for each specific language. The macro avg and weighted avg values provide an overall evaluation of the model's performance across all languages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
