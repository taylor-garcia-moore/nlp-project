{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import prepare\n",
    "from env import token, username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.custom_visual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c803d",
   "metadata": {},
   "source": [
    "> Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from env import token, username\n",
    "\n",
    "# GitHub API endpoint\n",
    "url = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    \"q\": \"stars:>0\",\n",
    "    \"sort\": \"stars\",\n",
    "    \"order\": \"desc\",\n",
    "    \"per_page\": 100,\n",
    "}\n",
    "\n",
    "# Authentication headers\n",
    "headers = {\"Authorization\": f\"token {token}\", \"User-Agent\": username}\n",
    "\n",
    "# Send a GET request to the GitHub API\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Get the response data\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Extract the repository names\n",
    "    repos = [repo[\"full_name\"] for repo in response_data[\"items\"]]\n",
    "\n",
    "    # Display the repository names\n",
    "    print(repos)\n",
    "else:\n",
    "    print(\"Failed to retrieve repository names from the GitHub API\")\n",
    "    \n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert repos list to DataFrame\n",
    "df_repos = pd.DataFrame({'Repository': repos})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a62a45",
   "metadata": {},
   "source": [
    "> Retrieved 100 of the most starred repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from env import token, username\n",
    "\n",
    "# List of repository names\n",
    "#df_repos = repos\n",
    "#repos = ['trekhleb/javascript-algorithms', 'ohmyzsh/ohmyzsh', 'TheAlgorithms/Python', 'torvalds/linux', 'Snailclimb/JavaGuide', 'huggingface/transformers', 'trimstray/the-book-of-secret-knowledge', 'microsoft/PowerToys', 'rust-lang/rust', 'f/awesome-chatgpt-prompts', 'ripienaar/free-for-dev', 'bitcoin/bitcoin', 'spring-projects/spring-boot', 'netdata/netdata', 'coder/code-server', 'reduxjs/redux', 'atom/atom', 'protocolbuffers/protobuf', 'angular/angular.js', 'keras-team/keras', 'h5bp/Front-end-Developer-Interview-Questions', 'nestjs/nest', 'lodash/lodash', 'leonardomso/33-js-concepts', 'apache/echarts', 'rails/rails', 'MunGell/awesome-for-beginners', 'scutan90/DeepLearning-500-questions', 'supabase/supabase', 'google/material-design-icons', 'nuxt/nuxt', 'serverless/serverless', 'ryanoasis/nerd-fonts', 'meteor/meteor', 'cypress-io/cypress', 'Textualize/rich', 'iamkun/dayjs', 'ColorlibHQ/AdminLTE', 'parcel-bundler/parcel', 'ethereum/go-ethereum', 'styled-components/styled-components', 'youngyangyang04/leetcode-master', 'discourse/discourse', 'agalwood/Motrix', 'hashicorp/terraform', 'vuetifyjs/vuetify', 'FFmpeg/FFmpeg', 'serhii-londar/open-source-mac-os-apps', 'jesseduffield/lazygit', 'wg/wrk', 'ant-design/ant-design-pro', 'eugenp/tutorials', 'microsoft/monaco-editor', 'google/leveldb', 'LAION-AI/Open-Assistant', 'gto76/python-cheatsheet', 'alibaba/arthas', 'xitu/gold-miner', 'facebookresearch/segment-anything', 'psf/black', 'geekxh/hello-algorithm', 'pyenv/pyenv', 'withastro/astro', 'rapid7/metasploit-framework', 'mqyqingfeng/Blog', 'typicode/husky', 'lib-pku/libpku', 'bradtraversy/50projects50days', 'PKUanonym/REKCARC-TSC-UHT', 'ccxt/ccxt', 'jashkenas/backbone', 'solidjs/solid', 'postcss/postcss', 'cockroachdb/cockroach', 'fzaninotto/Faker', 'danielgindi/Charts', 'ibraheemdev/modern-unix', 'realpython/python-guide', 'nvie/gitflow', 'geekcompany/ResumeSample', 'explosion/spaCy', 'ehang-io/nps', 'ajaxorg/ace', 'encode/django-rest-framework', 'viraptor/reverse-interview', 'AllThingsSmitty/css-protips', 'yichengchen/clashX', 'ggreer/the_silver_searcher', 'Ebazhanov/linkedin-skill-assessments-quizzes', 'facebookresearch/fastText', 'akveo/ngx-admin', 'open-mmlab/mmdetection', 'pypa/pipenv', 'hammerjs/hammer.js', 'VundleVim/Vundle.vim', 'quasarframework/quasar', 'zhiwehu/Python-programming-exercises', 'qier222/YesPlayMusic', 'remix-run/remix', 'dnSpy/dnSpy']\n",
    "\n",
    "\n",
    "# Scrape README files and store their content in a dictionary\n",
    "readme_data = {'Repository': [], 'Readme': []}\n",
    "for repo in repos:\n",
    "    # Get the URL of the README file\n",
    "    url = f\"https://api.github.com/repos/{repo}/readme\"\n",
    "\n",
    "    # Send a GET request to the URL with authentication\n",
    "    headers = {\"Authorization\": f\"token {token}\", \"User-Agent\": username}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the content from the response\n",
    "        response_data = response.json()\n",
    "        readme_content = response_data['content']\n",
    "\n",
    "        # Decode the base64 encoded content\n",
    "        import base64\n",
    "        readme_text = base64.b64decode(readme_content).decode('utf-8')\n",
    "\n",
    "        # Store the repository name and the corresponding README content\n",
    "        readme_data['Repository'].append(repo)\n",
    "        readme_data['Readme'].append(readme_text)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve README for repository: {repo}\")\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(readme_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)\n",
    "\n",
    "# Print the notification message\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Our data comes from the top 100 most_starred repositories on GitHub as of {current_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5399f",
   "metadata": {},
   "source": [
    "Performs web scraping to gather README files from a list of GitHub repositories. Let's go through the code and its output step by step:\n",
    "\n",
    "The code imports the required libraries and modules: datetime, requests, pandas, and the token and username variables from the env.py file.\n",
    "The repos list contains the names of the repositories from which we want to scrape README files.\n",
    "The readme_data dictionary is initialized to store the repository names and their corresponding README contents.\n",
    "8-19. The code iterates over each repository in the repos list. It constructs the URL for the repository's README file using the GitHub API. It sends a GET request to the URL with the necessary authentication headers. If the response is successful (status code 200), it extracts the content of the README file.\n",
    "\n",
    "12-15. The base64-encoded content is decoded and converted to UTF-8 format to obtain the plain text of the README file.\n",
    "\n",
    "18-20. The repository name and the corresponding README content are appended to the readme_data dictionary.\n",
    "\n",
    "23-26. Once all the repositories have been processed, the readme_data dictionary is converted into a DataFrame using the pd.DataFrame() function.\n",
    "\n",
    "The DataFrame is saved to a CSV file named 'readme_data.csv' using the to_csv() method.\n",
    "32-34. The code prints a notification message indicating the date when the data was collected. The datetime.now() function retrieves the current date, which is then formatted as \"%Y-%m-%d\" (e.g., \"2023-05-14\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24537569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86094b49",
   "metadata": {},
   "source": [
    "performs various operations related to scraping and analyzing README files from GitHub repositories. Let's go through the code and its output step by step:\n",
    "\n",
    "1-4: The code imports the required libraries and modules: requests, pandas, re, Counter, and the token and username variables from the env.py file.\n",
    "\n",
    "7-16: The code defines the GitHub API endpoint and sets the parameters for the API request. These parameters specify the search query, sorting criteria, order, and number of repositories to retrieve.\n",
    "\n",
    "18-21: Authentication headers are set using the token and username variables to authorize the request.\n",
    "\n",
    "24-29: The code sends a GET request to the GitHub API with the specified URL, parameters, and authentication headers.\n",
    "\n",
    "32-40: The response status code is checked to ensure that the request was successful (status code 200). If successful, the repository names are extracted from the response data.\n",
    "\n",
    "43-54: A list of programming-related words is defined. These words represent common programming languages.\n",
    "\n",
    "57-59: A dictionary called word_categories is created to categorize words by programming language. Each word is initially associated with an empty list.\n",
    "\n",
    "62-80: The code iterates over each repository in the retrieved list of repositories. For each repository, it constructs the URL for the README file using the GitHub API. It sends a GET request to the URL with the necessary authentication headers. If the response is successful, it extracts the content of the README file.\n",
    "\n",
    "83-89: The base64-encoded content is decoded and converted to UTF-8 format to obtain the plain text of the README file.\n",
    "\n",
    "92-99: The code uses regular expressions (re.findall()) to extract individual words from the README text. These words are converted to lowercase.\n",
    "\n",
    "102-106: The words are categorized by programming language. If a word exists in the word_categories dictionary, the repository is added to the corresponding list.\n",
    "\n",
    "109-112: The code finds the most common words in the READMEs using the Counter class from the collections module. The most_common() method is used to retrieve the most common words and their counts.\n",
    "\n",
    "115-121: The most common words are displayed, along with their counts.\n",
    "\n",
    "The output of this code will be the most common words found in the READMEs, along with their respective frequencies. For example:\n",
    "\n",
    "Most common words in READMEs:\n",
    "the: 166\n",
    "cheat: 149\n",
    "sh: 146\n",
    "you: 115\n",
    "a: 101\n",
    "to: 93\n",
    "cht: 81\n",
    "and: 62\n",
    "in: 62\n",
    "https: 60\n",
    "\n",
    "These words indicate the frequency at which they appear in the README files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    headers = {\"Authorization\": f\"token {token}\", \"User-Agent\": username}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        repo_info = response.json()\n",
    "        if \"language\" in repo_info:\n",
    "            language = repo_info[\"language\"]\n",
    "            return language\n",
    "    return \"\"\n",
    "\n",
    "for repo in repos:\n",
    "    language = get_repo_language(repo)\n",
    "    if language:\n",
    "        print(f\"Repository: {repo}, Language: {language}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve language for repository: {repo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the languages\n",
    "languages = []\n",
    "\n",
    "# Loop through each repository in the DataFrame\n",
    "for repo in df['Repository']:\n",
    "    # Get the language for the repository\n",
    "    language = get_repo_language(repo)\n",
    "    # Append the language to the list\n",
    "    languages.append(language)\n",
    "\n",
    "# Create a new 'Language' column in the DataFrame\n",
    "df['Language'] = languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96047ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary to categorize words by programming language\n",
    "word_categories = {word: [] for word in languages}\n",
    "\n",
    "# Create a dictionary to store the length of READMEs by language\n",
    "length_by_language = {}\n",
    "\n",
    "# Loop through each repository in the DataFrame\n",
    "for repo in df['Repository']:\n",
    "    # Get the URL of the README file\n",
    "    url = f\"https://api.github.com/repos/{repo}/readme\"\n",
    "\n",
    "    # Send a GET request to the URL with authentication\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the content from the response\n",
    "        response_data = response.json()\n",
    "        readme_content = response_data[\"content\"]\n",
    "\n",
    "        # Decode the base64 encoded content\n",
    "        import base64\n",
    "        readme_text = base64.b64decode(readme_content).decode(\"utf-8\")\n",
    "\n",
    "        # Extract words from the README text\n",
    "        words = re.findall(r\"\\b\\w+\\b\", readme_text.lower())\n",
    "\n",
    "        # Calculate the length of the README\n",
    "        length = len(readme_text)\n",
    "\n",
    "        # Update length by language dictionary\n",
    "        language = get_repo_language(repo)\n",
    "        if language in length_by_language:\n",
    "            length_by_language[language].append(length)\n",
    "        else:\n",
    "            length_by_language[language] = [length]\n",
    "\n",
    "        # Categorize words by programming language\n",
    "        for word in words:\n",
    "            if word in word_categories:\n",
    "                word_categories[word].append(repo)\n",
    "\n",
    "# Find the most common words in READMEs\n",
    "common_words = Counter(words).most_common(10)\n",
    "\n",
    "# Display the most common words\n",
    "print(\"Most common words in READMEs:\")\n",
    "for word, count in common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Calculate the average length of READMEs by programming language\n",
    "average_length_by_language = {\n",
    "    language: sum(lengths) / len(lengths)\n",
    "    for language, lengths in length_by_language.items()\n",
    "}\n",
    "\n",
    "# Display the average length of READMEs by programming language\n",
    "print(\"Average README Length by Language:\")\n",
    "for language, length in average_length_by_language.items():\n",
    "    print(f\"{language}: {length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f83743",
   "metadata": {},
   "source": [
    "> Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8caf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [word for word, count in common_words]\n",
    "counts = [count for word, count in common_words]\n",
    "\n",
    "plt.barh(words, counts)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Most Common Words in READMEs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [word for word, count in common_words]\n",
    "counts = [count for word, count in common_words]\n",
    "\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most Common Words in READMEs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc2c29",
   "metadata": {},
   "source": [
    "> Clean Me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "readme_df = pd.DataFrame({\n",
    "    'Language': list(average_length_by_language.keys()),\n",
    "    'Average README Length': list(average_length_by_language.values())\n",
    "})\n",
    "\n",
    "# Merge the readme_df DataFrame with the df DataFrame\n",
    "df = df.merge(readme_df, on='Language', how='left')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c97290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b071246",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_length_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04085bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for analysis\n",
    "length_by_language = {}\n",
    "unique_words_by_language = {}\n",
    "unique_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faefca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = list(average_length_by_language.keys())\n",
    "lengths = list(average_length_by_language.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032af250",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out None values from languages and lengths\n",
    "languages_filtered = [lang for lang in languages if lang is not None]\n",
    "lengths_filtered = [length for lang, length in zip(languages, lengths) if lang is not None]\n",
    "\n",
    "plt.plot(languages_filtered, lengths_filtered, marker='o')\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Average README Length')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8009fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out None values from languages and lengths\n",
    "languages_filtered = [lang for lang in languages if lang is not None]\n",
    "lengths_filtered = [length for lang, length in zip(languages, lengths) if lang is not None]\n",
    "\n",
    "plt.bar(languages_filtered, lengths_filtered)\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Average README Length')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(languages_filtered, lengths_filtered)\n",
    "plt.xlabel('Average README Length')\n",
    "plt.ylabel('Programming Language')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3beda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(languages_filtered, lengths_filtered)\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Average README Length')\n",
    "plt.title('Average README Length by Language')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4263dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame to a CSV file\n",
    "df = pd.read_csv('readme_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de624644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the language is Python\n",
    "    if row['Language'] == 'Python':\n",
    "        # Assign 'Python' label to the 'target' column\n",
    "        df.at[index, 'Language'] = 'Python'\n",
    "    else:\n",
    "        # Assign 'Other' label to the 'target' column\n",
    "        df.at[index, 'Language'] = 'Other'\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbf574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary for the labels\n",
    "label_mapping = {'Python': 'Python', 'JavaScript': 'Other', 'Java': 'Other', 'C++': 'Other'}  # Add more languages as needed\n",
    "\n",
    "# Create the 'target' column by mapping the values from 'Language' column\n",
    "df['target'] = df['Language'].map(label_mapping)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbb03c",
   "metadata": {},
   "source": [
    "> Data Acquired! Cleaning started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e95013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Readme'] = df['Readme'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Readme'] = df['Readme'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numerical digits\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'\\d+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87301287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'<.*?>', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cleaned dataset\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f85f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the default set of stopwords\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Add prepositions to the stopwords set\n",
    "prepositions = ['about', 'above', 'across', 'after', 'against', 'along', 'among', 'around',\n",
    "                'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between',\n",
    "                'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down',\n",
    "                'during', 'except', 'for', 'from', 'in', 'inside', 'into', 'like',\n",
    "                'near', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over', 'past',\n",
    "                'regarding', 'round', 'since', 'through', 'throughout', 'to', 'toward',\n",
    "                'under', 'underneath', 'until', 'unto', 'up', 'upon', 'with', 'within',\n",
    "                'without','htttp','https','com']\n",
    "\n",
    "stopwords_with_prepositions = default_stopwords.union(prepositions)\n",
    "\n",
    "# Print the stopwords with prepositions\n",
    "print(stopwords_with_prepositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords removal to 'Readme' column\n",
    "stop = set(stopwords_with_prepositions)\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c310054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df['Readme']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df['Readme']).split()).value_counts()[-10:]\n",
    "freq = list(freq.index)\n",
    "df['Readme'] = df['Readme'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0625cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#st = PorterStemmer()\n",
    "#df['Readme'] = df['Readme'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lem = WordNetLemmatizer()\n",
    "#df['Readme'] = df['Readme'].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d13fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('readme_github.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80061c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out stop words from common words\n",
    "words = [word for word, count in common_words if word not in stopwords_with_prepositions]\n",
    "counts = [count for word, count in common_words if word not in stopwords_with_prepositions]\n",
    "\n",
    "plt.barh(words, counts)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Most Common Words in READMEs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dad9eb",
   "metadata": {},
   "source": [
    "> Ready for Explore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ad805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Drop rows with missing values in 'Language' column\n",
    "df = df.dropna(subset=['Language'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Remove rows with 'None' values from training data\n",
    "non_null_indices = y_train.notnull()\n",
    "X_train_vectors = X_train_vectors[non_null_indices]\n",
    "y_train = y_train[non_null_indices]\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report with zero_division=1\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e52a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68556e0f",
   "metadata": {},
   "source": [
    "Performs a text classification task using machine learning algorithms. The goal is to predict the programming language of a repository based on the contents of its README file.\n",
    "\n",
    "Here's an explanation of the code and the resulting classification report:\n",
    "\n",
    "The code starts by splitting the data into training and testing sets. The README content (df['Readme']) is used as the input features, and the corresponding programming language (df['Language']) is the target variable. The data is divided into 80% training set and 20% testing set.\n",
    "4-6. The TF-IDF vectorizer (TfidfVectorizer()) is created. This is a technique that converts text documents into numerical vectors, representing the importance of words in the documents.\n",
    "\n",
    "8-9. The vectorizer is fitted on the training data (X_train) to learn the vocabulary and calculate the TF-IDF weights for the words.\n",
    "\n",
    "11-12. The testing data (X_test) is transformed using the fitted vectorizer, resulting in numerical vectors that represent the README contents.\n",
    "\n",
    "14-15. A Support Vector Machine (SVM) classifier (SVC()) is created. SVM is a machine learning algorithm used for classification tasks.\n",
    "\n",
    "The SVM classifier is trained on the training data (X_train_vectors, y_train), where it learns patterns and relationships between the README contents and their corresponding programming languages.\n",
    "The trained SVM classifier is used to make predictions on the testing data (X_test_vectors).\n",
    "The classification_report function is called to generate a report that evaluates the performance of the classification. It calculates various metrics such as precision, recall, F1-score, and support for each programming language. The zero_division=1 parameter is set to handle the case where a programming language has no predicted samples.\n",
    "The resulting classification report shows the performance metrics for each programming language. Precision represents the accuracy of the predictions for each language, recall measures the coverage of the predictions, and F1-score is a combination of precision and recall. The support column indicates the number of samples in the testing set for each language.\n",
    "\n",
    "Overall, the classification report shows the performance of the model in predicting the programming languages based on the README contents. The accuracy is given as 0.79, which means that the model predicted the correct programming language for 79% of the repositories in the testing set. The precision, recall, and F1-score values vary for different programming languages, indicating how well the model performs for each specific language. The macro avg and weighted avg values provide an overall evaluation of the model's performance across all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927af34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame to a CSV file\n",
    "df = pd.read_csv('readme_github.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e030a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51178859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to extract the top 5 words from a given text\n",
    "def extract_top_words(text):\n",
    "    tokens = text.split()  # Split the text into individual words\n",
    "    word_counts = Counter(tokens)  # Count the frequencies of each word\n",
    "    top_words = [word for word, count in word_counts.most_common(10)]  # Extract the top 5 words\n",
    "    return top_words\n",
    "\n",
    "# Create a new column for the top 5 words\n",
    "df['Top_5_Words'] = df['Readme'].apply(extract_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the word count\n",
    "def calculate_word_count(text):\n",
    "    tokens = text.split()  # Split the text into individual words\n",
    "    word_count = len(tokens)  # Count the number of words\n",
    "    return word_count\n",
    "\n",
    "# Function to calculate the average word length\n",
    "def calculate_average_word_length(text):\n",
    "    tokens = text.split()  # Split the text into individual words\n",
    "    total_length = sum(len(word) for word in tokens)  # Sum the lengths of all words\n",
    "    word_count = len(tokens)  # Count the number of words\n",
    "    average_length = total_length / word_count  # Calculate the average word length\n",
    "    return average_length\n",
    "\n",
    "# Create new columns for word count and average word length\n",
    "df['Word_Count'] = df['Readme'].apply(calculate_word_count)\n",
    "df['Average_Word_Length'] = df['Readme'].apply(calculate_average_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import acquire, prepare, wrangle\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Readme'])\n",
    "y = df['Language']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the SVM model\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Print evaluation scores for SVM\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Train and evaluate the Naive Bayes model\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "\n",
    "# Print evaluation scores for Naive Bayes\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_test, nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ff874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: In order to make bigrams and trigrams, you need to have your text data in a list of words format.\n",
    "# If your text data is not already in this format, you will need to preprocess your data to convert it to this format.\n",
    "# Here, we assume your text data is in a column called 'text'.\n",
    "df['Readme'] = df['Readme'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the 'Readme' column into a string\n",
    "readme_text = [' '.join(words) for words in df['Readme']]\n",
    "\n",
    "# Create a CountVectorizer instance for bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the 'Readme' column for bigrams\n",
    "X_bigram = bigram_vectorizer.fit_transform(readme_text)\n",
    "\n",
    "# Get the feature names (bigrams)\n",
    "bigram_features = bigram_vectorizer.get_feature_names()\n",
    "\n",
    "# Create a CountVectorizer instance for trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Fit and transform the 'Readme' column for trigrams\n",
    "X_trigram = trigram_vectorizer.fit_transform(readme_text)\n",
    "\n",
    "# Get the feature names (trigrams)\n",
    "trigram_features = trigram_vectorizer.get_feature_names()\n",
    "\n",
    "# Print the bigrams and trigrams\n",
    "print(\"Bigrams:\")\n",
    "print(bigram_features)\n",
    "print(\"\\nTrigrams:\")\n",
    "print(trigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8732121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Convert the 'Readme' column into a string\n",
    "readme_text = [' '.join(words) for words in df['Readme']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c505ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer instance for bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_bigram = bigram_vectorizer.fit_transform(readme_text)\n",
    "\n",
    "# Get the feature names (bigrams)\n",
    "bigram_features = bigram_vectorizer.get_feature_names()\n",
    "\n",
    "# Calculate the frequencies of bigrams\n",
    "bigram_counts = X_bigram.sum(axis=0).A1\n",
    "bigram_freqs = dict(zip(bigram_features, bigram_counts))\n",
    "\n",
    "# Print the top 5 most frequent bigrams\n",
    "top_bigrams = Counter(bigram_freqs).most_common(10)\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, count in top_bigrams:\n",
    "    print(f\"{bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f439b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer instance for trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X_trigram = trigram_vectorizer.fit_transform(readme_text)\n",
    "\n",
    "# Get the feature names (trigrams)\n",
    "trigram_features = trigram_vectorizer.get_feature_names()\n",
    "\n",
    "# Calculate the frequencies of trigrams\n",
    "trigram_counts = X_trigram.sum(axis=0).A1\n",
    "trigram_freqs = dict(zip(trigram_features, trigram_counts))\n",
    "\n",
    "# Print the top 5 most frequent trigrams\n",
    "top_trigrams = Counter(trigram_freqs).most_common(10)\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for trigram, count in top_trigrams:\n",
    "    print(f\"{trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa054f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('readme_github.csv')\n",
    "\n",
    "# Remove numerical digits\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove stop words\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#df['Readme'] = df['Readme'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Remove URLs\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))\n",
    "\n",
    "# Remove HTML tags\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
    "\n",
    "# Remove special characters\n",
    "df['Readme'] = df['Readme'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "\n",
    "# Print the cleaned dataset\n",
    "print(df['Readme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6931fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('readme_github.csv')\n",
    "\n",
    "# Removing Duplicates\n",
    "df['Readme'] = df['Readme'].apply(lambda x: ' '.join(set(x.split())))\n",
    "\n",
    "# Handling Contractions\n",
    "def expand_contractions(text):\n",
    "    contraction_mapping = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        # Add more contractions and their expansions as needed\n",
    "    }\n",
    "    words = text.split()\n",
    "    expanded_words = [contraction_mapping[word.lower()] if word.lower() in contraction_mapping else word for word in words]\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "df['Readme'] = df['Readme'].apply(expand_contractions)\n",
    "\n",
    "# Handling Negations\n",
    "def split_negations(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    split_text = mark_negation(tokenized_text)\n",
    "    return ' '.join(split_text)\n",
    "\n",
    "df['Readme'] = df['Readme'].apply(split_negations)\n",
    "\n",
    "# Handling Abbreviations\n",
    "def expand_abbreviations(text):\n",
    "    abbreviation_mapping = {\n",
    "        \"USA\": \"United States of America\",\n",
    "        # Add more abbreviations and their expansions as needed\n",
    "    }\n",
    "    words = text.split()\n",
    "    expanded_words = [abbreviation_mapping[word] if word in abbreviation_mapping else word for word in words]\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "df['Readme'] = df['Readme'].apply(expand_abbreviations)\n",
    "\n",
    "# Removing Short or Long Words\n",
    "def remove_short_or_long_words(text, min_length, max_length):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if len(word) >= min_length and len(word) <= max_length]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['Readme'] = df['Readme'].apply(lambda x: remove_short_or_long_words(x, min_length=2, max_length=20))\n",
    "\n",
    "# Print the cleaned dataset\n",
    "print(df['Readme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data using the fitted vectorizer\n",
    "X_val_vectors = vectorizer.transform(X_val)\n",
    "\n",
    "# Train the models\n",
    "# Logistic Regression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Random Forest\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC()\n",
    "svm.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "logistic_regression_pred = logistic_regression.predict(X_val_vectors)\n",
    "random_forest_pred = random_forest.predict(X_val_vectors)\n",
    "svm_pred = svm.predict(X_val_vectors)\n",
    "naive_bayes_pred = naive_bayes.predict(X_val_vectors)\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_val, logistic_regression_pred))\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_val, random_forest_pred))\n",
    "print(\"SVM:\")\n",
    "print(classification_report(y_val, svm_pred))\n",
    "print(\"Naive Bayes:\")\n",
    "print(classification_report(y_val, naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a30598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Filter the dataset for Python and another programming language\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "other_df = df[df['Language'] != 'Python'].sample(len(python_df))\n",
    "\n",
    "# Combine the Python and other programming language datasets\n",
    "combined_df = pd.concat([python_df, other_df], ignore_index=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(combined_df['Readme'], combined_df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data using the fitted vectorizer\n",
    "X_val_vectors = vectorizer.transform(X_val)\n",
    "\n",
    "# Train the models\n",
    "# Logistic Regression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Random Forest\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC()\n",
    "svm.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "logistic_regression_pred = logistic_regression.predict(X_val_vectors)\n",
    "random_forest_pred = random_forest.predict(X_val_vectors)\n",
    "svm_pred = svm.predict(X_val_vectors)\n",
    "naive_bayes_pred = naive_bayes.predict(X_val_vectors)\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_val, logistic_regression_pred))\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_val, random_forest_pred))\n",
    "print(\"SVM:\")\n",
    "print(classification_report(y_val, svm_pred))\n",
    "print(\"Naive Bayes:\")\n",
    "print(classification_report(y_val, naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Set up the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_model.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19744c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Filter the dataset for Python and non-Python repositories\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "non_python_df = df[df['Language'] != 'Python']\n",
    "\n",
    "# Sample non-Python repositories to match the number of Python repositories\n",
    "non_python_df = non_python_df.sample(n=len(python_df), random_state=42)\n",
    "\n",
    "# Combine Python and non-Python datasets\n",
    "combined_df = pd.concat([python_df, non_python_df])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df['Readme'], combined_df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Set up the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_model.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to return value counts as a dataframe\n",
    "def value_counts_df(df, column):\n",
    "    absolute = pd.DataFrame(df[column].value_counts())\n",
    "    percent = pd.DataFrame(df[column].value_counts(normalize=True))\n",
    "    df_value_counts = pd.concat([absolute, percent], axis=1)\n",
    "    df_value_counts.columns = ['n', 'percent']\n",
    "    return df_value_counts\n",
    "\n",
    "# Apply value_counts_df to your current data\n",
    "df_value_counts = value_counts_df(df, 'Language')\n",
    "\n",
    "# Print the value counts dataframe\n",
    "print(df_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the value counts as a horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_value_counts['n'].plot(kind='barh')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Language')\n",
    "plt.title('Language Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Convert 'Readme' column to string\n",
    "df['Readme'] = df['Readme'].astype(str)\n",
    "\n",
    "# POS Tagging\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize(pos_tags):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word, pos in pos_tags:\n",
    "        # Convert POS tags to WordNet POS tags\n",
    "        wn_pos = get_wordnet_pos(pos)\n",
    "        if wn_pos:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# Helper function to convert POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply POS tagging and lemmatization to your dataset\n",
    "df['POS_tags'] = df['Readme'].apply(pos_tagging)\n",
    "df['Lemmas'] = df['POS_tags'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25751227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09984e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Filter the dataset for Python and non-Python repositories\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "non_python_df = df[df['Language'] != 'Python']\n",
    "\n",
    "# Sample non-Python repositories to match the number of Python repositories\n",
    "non_python_df = non_python_df.sample(n=len(python_df), random_state=42)\n",
    "\n",
    "# Combine Python and non-Python datasets\n",
    "combined_df = pd.concat([python_df, non_python_df])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df['Readme'], combined_df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Set up the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'kernel': ['linear'],\n",
    "    'C': [0.1],\n",
    "    'gamma': [0.1]\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_model.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "# Create a BytePairBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Get the texts from the 'Readme' column\n",
    "texts = df['Readme'].tolist()\n",
    "\n",
    "# Train the tokenizer on the texts\n",
    "tokenizer.train_from_iterator(texts, show_progress=True)\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = [tokenizer.encode(text).tokens for text in texts]\n",
    "\n",
    "# Add the tokenized texts back to the DataFrame\n",
    "df['Tokenized_Readme'] = tokenized_texts\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save('bpe_tokenizer.json')\n",
    "\n",
    "# Save the DataFrame with tokenized texts\n",
    "df.to_csv('tokenized_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98da561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Filter the dataset for Python and non-Python repositories\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "non_python_df = df[df['Language'] != 'Python']\n",
    "\n",
    "# Sample non-Python repositories to match the number of Python repositories\n",
    "non_python_df = non_python_df.sample(n=len(python_df), random_state=42)\n",
    "\n",
    "# Combine Python and non-Python datasets\n",
    "combined_df = pd.concat([python_df, non_python_df])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df['Readme'], combined_df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Set up the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_model.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ba183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Tokenized_Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the tokenized texts to string\n",
    "X_train = X_train.apply(lambda tokens: ' '.join(tokens))\n",
    "X_test = X_test.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Vectorize the tokenized texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classification model\n",
    "svm = SVC()\n",
    "svm.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = svm.score(X_test_vectors, y_test)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79435b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de16221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Filter the dataset for Python and non-Python repositories\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "non_python_df = df[df['Language'] != 'Python']\n",
    "\n",
    "# Sample non-Python repositories to match the number of Python repositories\n",
    "non_python_df = non_python_df.sample(n=len(python_df), random_state=42)\n",
    "\n",
    "# Combine Python and non-Python datasets\n",
    "combined_df = pd.concat([python_df, non_python_df])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the bag of words representation\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "# Create the TF-IDF representation\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# Train and evaluate each model using bag of words representation\n",
    "for model in models:\n",
    "    model.fit(X_train_bow, y_train)\n",
    "    y_pred = model.predict(X_test_bow)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {model.__class__.__name__} - Bag of Words Accuracy: {accuracy}\")\n",
    "\n",
    "# Train and evaluate each model using TF-IDF representation\n",
    "for model in models:\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {model.__class__.__name__} - TF-IDF Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96104a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Filter the dataset for Python and non-Python repositories\n",
    "python_df = df[df['Language'] == 'Python']\n",
    "non_python_df = df[df['Language'] != 'Python']\n",
    "\n",
    "# Sample non-Python repositories to match the number of Python repositories\n",
    "non_python_df = non_python_df.sample(n=len(python_df), random_state=42)\n",
    "\n",
    "# Combine Python and non-Python datasets\n",
    "combined_df = pd.concat([python_df, non_python_df])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the bag of words representation\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "# Create the TF-IDF representation\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "# Train and evaluate each model using bag of words representation\n",
    "for model in models:\n",
    "    model.fit(X_train_bow, y_train)\n",
    "    y_pred = model.predict(X_test_bow)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {model.__class__.__name__} - Bag of Words Accuracy: {accuracy}\")\n",
    "\n",
    "# Train and evaluate each model using TF-IDF representation\n",
    "for model in models:\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {model.__class__.__name__} - TF-IDF Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0489d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df['Readme'], df['Language'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer for bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "train_features = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# Transform the testing data\n",
    "test_features = vectorizer.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize the SVR or SVOR model\n",
    "svr_model = SVR(kernel='linear')  # You can choose different kernel functions like 'poly' or 'rbf'\n",
    "# For SVOR, you can use the SVOR class provided by scikit-learn, or you can use ordinal classification with the SVR model.\n",
    "\n",
    "# Set the appropriate parameters for the model\n",
    "# You can adjust hyperparameters like C (regularization parameter) and epsilon (tolerance for errors) to improve performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Perform one-hot encoding on the 'Language' column\n",
    "df = pd.get_dummies(df, columns=['Language'])\n",
    "\n",
    "# Print the updated dataframe with one-hot encoded columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd5a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20819cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
